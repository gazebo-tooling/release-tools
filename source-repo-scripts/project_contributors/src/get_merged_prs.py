# Copyright 2025 Open Source Robotics Foundation
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# This file was generated by Gemini 2.5 Pro.

"""
A script to find all merged pull requests in a GitHub organization, bypassing
the 1000-item search limit.

Description:
This script works around the GitHub API's 1000-result search limit by breaking the
requested date range into quarterly (3-month) chunks. For each chunk, it performs
a paginated GraphQL search to fetch all merged PRs. The results are then
aggregated and filtered into a single final JSON array.

Usage:
    ./get_merged_prs.py <ORG_NAME> <START_DATE> [END_DATE]

Example:
    # Get all PRs merged in the 'gazebosim' org during the first half of 2024
    ./get_merged_prs.py gazebosim 2024-01-01 2024-06-30 > gazebo-prs.json

    # Get all PRs merged since the start of 2025 (end date defaults to today)
    ./get_merged_prs.py gazebosim 2025-01-01 > gazebo-prs-since.json

Dependencies:
    - gh (the GitHub CLI): https://cli.github.com/
"""

import sys
import json
import shutil
import subprocess
import argparse
from datetime import datetime, date, timedelta

# The GraphQL query to fetch merged PRs.
GQL_QUERY = """
query($searchQuery: String!, $afterCursor: String) {
  search(query: $searchQuery, type: ISSUE, first: 100, after: $afterCursor) {
    pageInfo {
      hasNextPage
      endCursor
    }
    edges {
      node {
        ... on PullRequest {
          url
          title
          author { login }
          repository { nameWithOwner }
          createdAt
          closedAt # Using closedAt, which is the merge time for merged PRs
          baseRefName
        }
      }
    }
  }
}
"""

def check_dependencies():
    """Checks if the 'gh' command is available in the system's PATH."""
    if not shutil.which("gh"):
        print(
            "Error: 'gh' command not found. "
            "Please install the GitHub CLI.",
            file=sys.stderr
        )
        sys.exit(1)

def parse_date(date_str: str) -> date:
    """Helper function to parse YYYY-MM-DD strings into date objects."""
    try:
        return datetime.strptime(date_str, "%Y-%m-%d").date()
    except ValueError:
        print(
            f"Error: Invalid date format '{date_str}'. "
            "Please use YYYY-MM-DD.",
            file=sys.stderr
        )
        sys.exit(1)

def fetch_prs_for_range(
    org_name: str,
    chunk_start: date,
    chunk_end: date
) -> list:
    """
    Fetches all pages of PRs for a specific date range (a "chunk").
    This function handles pagination within its given date range.
    """
    all_prs_in_range = []
    end_cursor = None

    # Use 'closed' date filter as it corresponds to the 'closedAt' field
    search_query = (
        f"is:pr is:merged org:{org_name} "
        f"closed:{chunk_start.isoformat()}..{chunk_end.isoformat()}"
    )
    print(
        f"Fetching PRs for range: {chunk_start} to {chunk_end}",
        file=sys.stderr
    )

    while True:
        # Build the 'gh api' command
        command = [
            "gh", "api", "graphql",
            "-f", f"query={GQL_QUERY}",
            "-f", f"searchQuery={search_query}"
        ]
        if end_cursor:
            command.extend(["-f", f"afterCursor={end_cursor}"])

        try:
            # Execute the command
            result = subprocess.run(
                command,
                capture_output=True,
                text=True,
                check=True,
                encoding="utf-8"
            )
            response = json.loads(result.stdout)

            if "errors" in response:
                print(
                    f"Error in GraphQL query: {response['errors']}",
                    file=sys.stderr
                )
                break

            search_data = response.get("data", {}).get("search", {})
            edges = search_data.get("edges", [])

            # Add valid nodes to our list
            for edge in edges:
                if node := edge.get("node"):
                    all_prs_in_range.append(node)

            page_info = search_data.get("pageInfo", {})
            has_next_page = page_info.get("hasNextPage", False)

            if not has_next_page:
                break  # Exit loop if no more pages

            end_cursor = page_info.get("endCursor")

        except subprocess.CalledProcessError as e:
            print(
                f"Error calling 'gh api': {e.stderr}",
                file=sys.stderr
            )
            break
        except json.JSONDecodeError as e:
            print(
                f"Error decoding JSON response from 'gh api': {e}",
                file=sys.stderr
            )
            break

    return all_prs_in_range

def filter_pr(pr: dict) -> bool:
    """
    Filters out unwanted pull requests based on defined criteria.
    Returns True if the PR should be KEPT, False if it should be FILTERED OUT.
    
    Current filters (all are case-insensitive where applicable):
    - Author is 'mergify'
    - Title contains 'backport'
    - Title contains 'Merge '
    """
    author = pr.get("author", {}).get("login")
    title = pr.get("title", "").lower() # Convert to lowercase once for efficiency

    if author == "mergify":
        return False

    if author == "renovate":
        return False
        
    if "backport" in title:
        return False
        
    if "merge " in title:
        return False

    # This is a good PR, keep it
    return True

def main():
    """
    Main function to parse arguments and orchestrate the PR fetching.
    """
    check_dependencies()

    parser = argparse.ArgumentParser(
        description=(
            "Find all merged PRs in a GitHub org, "
            "bypassing the 1000-item search limit."
        )
    )
    parser.add_argument(
        "org_name",
        help="GitHub organization name (e.g., 'gazebosim')"
    )
    parser.add_argument(
        "start_date",
        type=parse_date,
        help="Start date in YYYY-MM-DD format"
    )
    parser.add_argument(
        "end_date",
        nargs="?",
        default=date.today(),
        type=parse_date,
        help="End date in YYYY-MM-DD format (defaults to today)"
    )
    args = parser.parse_args()

    # Ensure start_date is not after end_date
    if args.start_date > args.end_date:
        print(
            "Error: Start date cannot be after end date.",
            file=sys.stderr
        )
        sys.exit(1)

    all_merged_prs = []
    current_date = args.start_date

    # Iterate from the start date to the end date, quarter by quarter
    while current_date <= args.end_date:
        chunk_start = current_date
        
        # Get the first day of the current month
        month_start = chunk_start.replace(day=1)
        
        # Calculate the start of the next 3-month period
        next_period_month_raw = month_start.month + 3
        next_period_year = (
            month_start.year + (next_period_month_raw - 1) // 12
        )
        next_period_month = (next_period_month_raw - 1) % 12 + 1
        
        next_period_start = date(next_period_year, next_period_month, 1)

        # The end of the current quarter is one day before the next period starts
        quarter_end = next_period_start - timedelta(days=1)

        # Ensure the chunk's end doesn't go past the overall END_DATE
        chunk_end = min(quarter_end, args.end_date)

        # Fetch PRs for this chunk
        prs_in_chunk = fetch_prs_for_range(
            args.org_name,
            chunk_start,
            chunk_end
        )
        all_merged_prs.extend(prs_in_chunk)

        # Move to the first day of the next 3-month period
        current_date = next_period_start

    # Filter the aggregated PRs using our filter function
    filtered_prs_list = [pr for pr in all_merged_prs if filter_pr(pr)]

    print(f"Total PRs fetched: {len(all_merged_prs)}", file=sys.stderr)
    print(f"Total PRs after filtering: {len(filtered_prs_list)}", file=sys.stderr)

    # Format the final list of PRs, similar to the original jq filter
    formatted_prs = [
        {
            "url": pr.get("url"),
            "title": pr.get("title"),
            "author": pr.get("author", {}).get("login"),
            "repository": pr.get("repository", {}).get("nameWithOwner"),
            "createdAt": pr.get("createdAt"),
            "mergedAt": pr.get("closedAt"), # Rename for clarity
            "baseRefName": pr.get("baseRefName")
        }
        for pr in filtered_prs_list # Use the filtered list here
    ]

    # Print the final aggregated JSON array to stdout
    print(json.dumps(formatted_prs, indent=2))

if __name__ == "__main__":
    main()